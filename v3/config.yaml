# VideoMAE Baseline Configuration

# Model configuration
model:
  pretrained: "MCG-NJU/videomae-base"  # Use pretrained weights
  mask_ratio: 0.9  # 90% masking ratio (VideoMAE standard)
  norm_pix_loss: true  # Normalize pixel loss

# Data configuration
data:
  data_root: "../smbdataset/data-smb"  # Path to SMB dataset
  image_size: 224  # Input image size
  num_frames: 16  # Number of frames per clip
  num_workers: 4  # Number of data loading workers

# Training configuration
training:
  num_epochs: 200  # Reduced from 800 for faster experimentation
  batch_size: 8  # Adjust based on GPU memory
  learning_rate: 1.5e-4  # Base learning rate
  min_lr: 1e-6  # Minimum learning rate for cosine schedule
  weight_decay: 0.05  # Weight decay for AdamW
  gradient_clip: 1.0  # Gradient clipping value
  seed: 42  # Random seed for reproducibility
  val_interval: 10  # Validate every N epochs
  save_interval: 50  # Save checkpoint every N epochs
  checkpoint_dir: "results/checkpoints"

# Evaluation configuration
evaluation:
  batch_size: 4  # Batch size for evaluation
  num_visualizations: 10  # Number of samples to visualize
  visualization_dir: "results/visualizations"

# Logging configuration
logging:
  tensorboard_dir: "results/tensorboard"
  log_interval: 10  # Log every N batches
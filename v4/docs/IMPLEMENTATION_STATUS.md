# SGAPS-MAE v4 Implementation Status

**Last Updated:** December 8, 2025  
**Current Phase:** Phase 1 Complete ‚Üí Phase 2 Starting

---

## Executive Summary

The SGAPS-MAE project is currently in **Phase 1 (Foundation)** with the core infrastructure **fully implemented** on both server and client sides. The system provides a working end-to-end pipeline for real-time frame capture, pixel sampling, WebSocket communication, and data storage. However, the advanced features (MAE model, adaptive sampling, attention-based importance) are **not yet implemented** and remain in the planning stage.

**Project Completion: ~30% (Infrastructure Complete, Core ML Features Pending)**

---

## üìä Phase-by-Phase Progress

| Phase       | Timeline | Progress    | Status          | Description                                       |
| ----------- | -------- | ----------- | --------------- | ------------------------------------------------- |
| **Phase 0** | Week 1-2 | 80%         | ‚ö†Ô∏è Partial      | Documentation written but ahead of implementation |
| **Phase 1** | Week 3-4 | ‚úÖ **100%** | ‚úÖ **COMPLETE** | Server/client communication, data collection      |
| **Phase 2** | Week 5-6 | ‚ùå **0%**   | üî¥ Not Started  | Adaptive sampling, importance calculation         |
| **Phase 3** | Week 7-8 | ‚ùå **0%**   | üî¥ Not Started  | SPT model, training pipeline                      |
| **Phase 4** | Week 9+  | ‚ùå **0%**   | üî¥ Not Started  | Advanced features, optimization                   |

---

## ‚úÖ PHASE 1: FULLY IMPLEMENTED FEATURES

### 1.1 Backend Infrastructure (Python FastAPI)

#### Communication Layer

-   ‚úÖ **WebSocket Server** (`sgaps/api/websocket.py`)

    -   Full endpoint implementation: `/ws/stream`
    -   Session management with per-client tracking
    -   Message routing: `session_start`, `frame_data`, `uv_coordinates`, `heartbeat`
    -   Error handling and graceful disconnection
    -   Connection manager with active connection tracking

-   ‚úÖ **REST API** (`sgaps/api/rest.py`)

    -   Health check endpoint: `GET /api/health`
    -   Status endpoint: `GET /api/status`
    -   Statistics tracking: uptime, active sessions, frames processed
    -   Readiness checks for deployment

-   ‚úÖ **CORS Middleware**
    -   Allow all origins (development mode)
    -   Proper header configuration
    -   Credential support

#### Session Management

-   ‚úÖ **Session Manager** (`sgaps/core/session_manager.py`)
    -   Complete session lifecycle management
    -   Per-client metadata: checkpoint_key, frame_count, duration
    -   Session creation, retrieval, update, cleanup
    -   Automatic resource cleanup on disconnect
    -   Thread-safe session access

#### Data Processing

-   ‚úÖ **HDF5 Storage** (`sgaps/data/storage.py`)

    -   Organized by checkpoint_key and session_id
    -   Hierarchical structure: `/checkpoint_key/session_id/frames/`
    -   Stores: pixels (N√ó3), state_vector, resolution, timestamps
    -   Asynchronous storage with periodic flushing
    -   Automatic file management and buffering
    -   Proper cleanup and resource management

-   ‚úÖ **PyTorch Dataset** (`sgaps/data/dataset.py`)
    -   Loads from HDF5 files
    -   Creates sparse images and masks
    -   Handles variable-length pixel arrays
    -   Custom collate function for DataLoader
    -   Supports multiple checkpoint keys

#### Sampling & Reconstruction

-   ‚úÖ **Fixed Sampler** (`sgaps/core/sampler.py`)

    -   Patterns: `uniform_grid`, `random`, `stratified`
    -   Grid calculation respects aspect ratio
    -   Configurable sample count
    -   UV coordinate generation (0.0-1.0 normalized)

-   ‚úÖ **Baseline Reconstructor** (`sgaps/core/reconstructor.py`)

    -   OpenCV inpainting: Telea and Navier-Stokes methods
    -   Quality metrics: MSE, PSNR, SSIM
    -   Serves as Phase 1 baseline for comparison
    -   Checkpoint loading stub (returns False, no actual model)

-   ‚úÖ **Metrics Module** (`sgaps/utils/metrics.py`)
    -   MSE calculation
    -   PSNR calculation
    -   SSIM calculation (simplified, production should use scikit-image)

#### Configuration System

-   ‚úÖ **Hydra Configuration** (`conf/`)

    -   `config.yaml`: Global settings
        -   `max_state_dim: 64`
        -   `sentinel_value: -999.0`
        -   `target_fps: 10`
    -   `server/development.yaml`: Server settings
        -   `host: 0.0.0.0`
        -   `port: 8000`
        -   `log_level: debug`
    -   `sampling/uniform.yaml`: Sampling patterns
        -   `pattern: uniform_grid`
        -   `default_sample_count: 500`
    -   `mask_update/fixed.yaml`: Update strategy
        -   `strategy: fixed`
        -   `update_interval: 0`

-   ‚úÖ **Server-Controlled Parameters**
    -   `sample_count`, `max_state_dim`, `target_fps` sent in `session_start_ack`
    -   Client receives and applies server configuration
    -   No hardcoded values in client

#### Server Entry Point

-   ‚úÖ **Main Application** (`main.py`)
    -   FastAPI app with Hydra integration
    -   Configuration loading and merging
    -   Router setup (REST + WebSocket)
    -   CORS middleware configuration
    -   Lifespan management (startup/shutdown)
    -   Can run with `uvicorn` or `python main.py`

### 1.2 Unity Client (UPM Package)

#### Package Structure

-   ‚úÖ **UPM Package Definition** (`package.json`)
    -   Package name: `com.sgaps.client`
    -   Version: `0.1.0`
    -   Unity version: `2021.3`
    -   Dependencies: `com.unity.nuget.newtonsoft-json`
    -   Assembly definitions for Runtime and Editor

#### Frame Capture

-   ‚úÖ **FrameCaptureHandler** (`Runtime/Scripts/Core/FrameCaptureHandler.cs`)

    -   Uses `ScreenCapture.CaptureScreenshotIntoRenderTexture()` API
    -   Captures final rendered screen including UI and post-processing
    -   Grayscale conversion via custom shader
    -   Automatic RenderTexture resizing on resolution change
    -   Must be called after `WaitForEndOfFrame` (correctly implemented)
    -   Proper resource cleanup

-   ‚úÖ **Grayscale Shader** (`Runtime/Shaders/GrayscaleConvert.shader`)
    -   ITU-R BT.601 luminance conversion
    -   `Y = 0.299*R + 0.587*G + 0.114*B`
    -   Optimized for RenderTexture processing

#### Pixel Sampling

-   ‚úÖ **PixelSampler** (`Runtime/Scripts/Core/PixelSampler.cs`)
    -   Generates initial patterns: UniformGrid, Random, Stratified, Checkerboard
    -   Efficient pixel extraction using `GetPixels32()` for fast sampling
    -   UV coordinate application from server
    -   Readback texture management with proper cleanup
    -   Handles resolution changes dynamically

#### Network Communication

-   ‚úÖ **NetworkClient** (`Runtime/Scripts/Core/NetworkClient.cs`)
    -   WebSocket client using `NativeWebSocket` library
    -   Async/await pattern for connection
    -   Message serialization with Newtonsoft.Json
    -   Event-driven architecture:
        -   `OnConnected`, `OnDisconnected`
        -   `OnSessionStarted`, `OnSessionError`
        -   `OnUVCoordinatesReceived`
        -   `OnHeartbeatAck`
    -   Main thread dispatcher for Unity callback safety
    -   Proper error handling and reconnection support
    -   Heartbeat mechanism for connection health

#### Data Structures

-   ‚úÖ **PixelData** (`Runtime/Scripts/Data/PixelData.cs`)

    -   Pixel representation with UV (Vector2) and value (float)
    -   Serializable for network transmission

-   ‚úÖ **UVCoordinates** (`Runtime/Scripts/Data/UVCoordinates.cs`)

    -   Container for UV coordinate arrays
    -   Efficient serialization

-   ‚úÖ **SessionConfig** (`Runtime/Scripts/Data/SessionConfig.cs`)

    -   Server configuration received from `session_start_ack`
    -   Fields: `sample_count`, `max_state_dim`, `target_fps`

-   ‚úÖ **Messages** (`Runtime/Scripts/Data/Messages.cs`)
    -   Complete message definitions for wire protocol:
        -   `SessionStartMessage`
        -   `SessionStartAckMessage`
        -   `FrameDataMessage`
        -   `UVCoordinatesMessage`
        -   `HeartbeatMessage`
        -   `ErrorMessage`

#### State Collection

-   ‚úÖ **StateVectorCollector** (`Runtime/Scripts/Data/StateVectorCollector.cs`)
    -   Variable-length state vector support
    -   Sentinel value support (default: -999.0)
    -   Only transmits used portion (not full max_state_dim)
    -   Auto-reset after each frame
    -   AddValue(), Clear(), ToArray() methods

#### Main Manager

-   ‚úÖ **SGAPSManager** (`Runtime/Scripts/Core/SGAPSManager.cs`)
    -   Main MonoBehaviour orchestrator
    -   Coroutine-based capture loop with configurable FPS
    -   Inspector integration:
        -   Server endpoint configuration
        -   Checkpoint key setting
        -   Auto-connect option
    -   Server-controlled parameters (receives from `session_start_ack`)
    -   Lifecycle management:
        -   Initialization
        -   Connection handling
        -   Frame capture loop
        -   Cleanup on destroy
    -   Event callbacks for custom game integration

#### Performance Monitoring

-   ‚úÖ **PerformanceMonitor** (`Runtime/Scripts/Utilities/PerformanceMonitor.cs`)
    -   Rolling average tracking
    -   FPS monitoring
    -   Capture time tracking
    -   Network latency measurement
    -   Stopwatch-based precision timing

### 1.3 Unity Project (sgaps-mae-fps)

-   ‚úÖ **UPM Package Integration**

    -   unity-client package imported
    -   SGAPSManager component available in scenes

-   ‚úÖ **Game Integration Ready**
    -   Can add SGAPSManager to any scene
    -   Compatible with existing game code
    -   Non-intrusive design

### 1.4 Protocol Implementation

-   ‚úÖ **Complete WebSocket Protocol**

    ```
    Client ‚Üí Server:
    - session_start {checkpoint_key, resolution, max_state_dim}
    - frame_data {frame_id, pixels[], state_vector, resolution, timestamp}
    - heartbeat {timestamp}

    Server ‚Üí Client:
    - session_start_ack {session_id, checkpoint_loaded, sample_count, max_state_dim, target_fps, model_version}
    - uv_coordinates {frame_id, coords[], importance_map}
    - heartbeat_ack {timestamp, server_time}
    - error {code, message}
    ```

-   ‚úÖ **Message Serialization**

    -   JSON-based serialization (Newtonsoft.Json)
    -   Proper handling of nested arrays
    -   Type-safe deserialization

-   ‚úÖ **Error Handling**
    -   Network errors caught and logged
    -   Connection loss detection
    -   Graceful degradation

---

## ‚ö†Ô∏è PARTIALLY IMPLEMENTED FEATURES

### Backend

#### Model Loading (Stub)

-   ‚ö†Ô∏è **`reconstructor.py`** - `load_checkpoint()`
    -   Currently always returns `False`
    -   No actual checkpoint loading implemented
    -   Stub in place for future model integration
    -   **Location:** `sgaps/core/reconstructor.py:41-45`

#### Metrics (Incomplete)

-   ‚ö†Ô∏è **SSIM Calculation**
    -   Simplified implementation
    -   Comment suggests using `scikit-image.metrics.structural_similarity` for production
    -   **Location:** `sgaps/utils/metrics.py:42-60`

### Client

#### Debug UI

-   ‚ö†Ô∏è **SGAPSDebugPanel**
    -   Mentioned in `CLIENT_IMPLEMENTATION.md`
    -   Not implemented in codebase
    -   Inspector shows connection status but no runtime debug panel

#### Editor Scripts

-   ‚ö†Ô∏è **Editor Assembly**
    -   Directory structure exists: `Editor/`
    -   `SGAPS.Editor.asmdef` defined
    -   No actual editor scripts implemented (only `SGAPSManagerEditor.cs` stub)

---

## ‚ùå PHASE 2: NOT YET IMPLEMENTED

### 2.1 Sparse Pixel Transformer (SPT) Model

**Critical: This is the core innovation of the project!**

#### Missing Files

-   ‚ùå `sgaps/models/` directory (doesn't exist)
-   ‚ùå `sgaps/models/__init__.py`
-   ‚ùå `sgaps/models/spt.py` - Main Transformer architecture
-   ‚ùå `sgaps/models/positional_encoding.py` - Continuous coordinate encoding
-   ‚ùå `sgaps/models/losses.py` - Loss functions

#### Required Components

**SparsePixelTransformer Class:**

```python
# sgaps/models/spt.py (TO BE IMPLEMENTED)

class SparsePixelTransformer(nn.Module):
    """
    Reconstructs full frames from sparse pixel sets using Transformer architecture.

    Architecture:
    1. Pixel Embedding: (N, 3) ‚Üí (N, embed_dim) with continuous positional encoding
    2. Self-Attention Encoder: Learn relationships between sparse pixels
    3. Query Grid: Generate (H√óW, embed_dim) queries for all pixel positions
    4. Cross-Attention Decoder: Query grid attends to sparse pixel features
    5. State Vector Encoder: Game state ‚Üí embedding with cross-attention
    6. CNN Refinement Head: Final reconstruction quality improvement

    Args:
        embed_dim: Embedding dimension (default: 256)
        num_heads: Number of attention heads (default: 8)
        num_encoder_layers: Self-attention layers (default: 6)
        num_decoder_layers: Cross-attention layers (default: 4)
        feedforward_dim: FFN dimension (default: 1024)
        dropout: Dropout rate (default: 0.1)
        max_state_dim: State vector dimension (default: 64)
        color_channels: Output channels (1 for grayscale, 3 for RGB)
    """

    def __init__(self, config):
        # Pixel embedding
        self.pixel_embed = nn.Linear(3, embed_dim)  # (u, v, value) ‚Üí embed_dim
        self.pos_encoding = ContinuousPositionalEncoding(embed_dim)

        # Self-attention encoder for sparse pixels
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=feedforward_dim,
            dropout=dropout,
            batch_first=True
        )
        self.sparse_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)

        # Cross-attention decoder
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=feedforward_dim,
            dropout=dropout,
            batch_first=True
        )
        self.cross_decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)

        # State vector encoder
        self.state_encoder = StateVectorEncoder(max_state_dim, embed_dim)

        # CNN refinement head
        self.refinement = nn.Sequential(
            nn.Conv2d(embed_dim, 256, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, color_channels, 1)
        )

    def forward(self, sparse_pixels, state_vector, state_mask, resolution):
        """
        Args:
            sparse_pixels: (B, N, 3) - (u, v, value)
            state_vector: (B, max_state_dim)
            state_mask: (B, max_state_dim) - 1 for valid, 0 for sentinel
            resolution: (height, width)

        Returns:
            reconstructed_frame: (B, C, H, W)
            attention_weights: (B, H*W, N) - for importance calculation
        """
        pass  # TO BE IMPLEMENTED
```

**Continuous Positional Encoding:**

```python
# sgaps/models/positional_encoding.py (TO BE IMPLEMENTED)

class ContinuousPositionalEncoding(nn.Module):
    """
    Sinusoidal positional encoding for continuous (u, v) coordinates.

    Unlike discrete patch-based encoding, this supports arbitrary pixel positions.
    """

    def __init__(self, embed_dim, max_freq=10):
        super().__init__()
        self.embed_dim = embed_dim
        self.max_freq = max_freq

    def forward(self, coords):
        """
        Args:
            coords: (B, N, 2) - (u, v) in [0, 1]

        Returns:
            pos_encoding: (B, N, embed_dim)
        """
        pass  # TO BE IMPLEMENTED
```

**Loss Functions:**

```python
# sgaps/models/losses.py (TO BE IMPLEMENTED)

class SampledPixelL2Loss(nn.Module):
    """
    Phase 1 MVP Loss: MSE on sampled pixels only.

    Formula:
        L = (1/N_sampled) * Œ£ ||I_pred(x_i) - I_gt(x_i)||¬≤

    where x_i are the sampled pixel positions.
    """

    def forward(self, pred_frame, gt_frame, sampled_coords):
        """
        Args:
            pred_frame: (B, C, H, W) - Reconstructed frame
            gt_frame: (B, C, H, W) - Ground truth frame
            sampled_coords: (B, N, 2) - Sampled (u, v) coordinates

        Returns:
            loss: Scalar tensor
        """
        pass  # TO BE IMPLEMENTED


class PerceptualLoss(nn.Module):
    """
    Phase 2+ Loss: LPIPS perceptual loss for non-sampled regions.
    """
    pass  # TO BE IMPLEMENTED


class StructuralLoss(nn.Module):
    """
    Phase 2+ Loss: SSIM-based structural consistency.
    """
    pass  # TO BE IMPLEMENTED
```

### 2.2 Adaptive Sampling System

#### Missing Files

-   ‚ùå `sgaps/core/importance.py` - Importance map calculation
-   ‚ùå `sgaps/core/mask_updater.py` - Mask update scheduling

#### Required Components

**Attention Entropy Importance Calculator:**

```python
# sgaps/core/importance.py (TO BE IMPLEMENTED)

class AttentionEntropyImportanceCalculator:
    """
    Calculates pixel importance using decoder cross-attention entropy.

    Key Insight:
    - Low entropy: Model focuses on specific sparse pixels ‚Üí confident ‚Üí low importance
    - High entropy: Model uncertain, spreads attention ‚Üí needs more samples ‚Üí high importance

    Formula:
        H(x) = -Œ£ p(x) * log(p(x))
        where p(x) is the attention weight distribution at position x
    """

    def __init__(self, config):
        self.epsilon = config.importance.epsilon  # 1e-9 for numerical stability

    def calculate(self, attention_weights, resolution):
        """
        Args:
            attention_weights: (B, H*W, N) - Cross-attention from decoder
            resolution: (height, width)

        Returns:
            importance_map: (B, H, W) - Higher values = more important
        """
        # Calculate entropy for each output position
        # H = -Œ£ p * log(p)
        eps = self.epsilon
        entropy = -(attention_weights * torch.log(attention_weights + eps)).sum(dim=-1)

        # Reshape to spatial dimensions
        B, HW = entropy.shape
        H, W = resolution
        importance_map = entropy.view(B, H, W)

        # Normalize to [0, 1]
        importance_map = (importance_map - importance_map.min()) / (importance_map.max() - importance_map.min() + eps)

        return importance_map
```

**Adaptive UV Sampler (Extension of existing sampler):**

```python
# sgaps/core/sampler.py (TO BE EXTENDED)

class AdaptiveUVSampler(FixedUVSampler):
    """
    Extends FixedUVSampler with importance-based weighted sampling.

    Strategy:
    - 60% importance-based: Sample high-entropy regions
    - 40% uniform: Maintain global coverage
    """

    def __init__(self, config):
        super().__init__(config)
        self.importance_ratio = 0.6

    def generate_adaptive_coords(self, importance_map, sample_count, prev_coords=None):
        """
        Args:
            importance_map: (H, W) - Importance scores
            sample_count: Number of pixels to sample
            prev_coords: (N, 2) - Previously sampled coordinates (optional)

        Returns:
            coords: (sample_count, 2) - New UV coordinates
        """
        H, W = importance_map.shape

        # Split budget
        importance_count = int(sample_count * self.importance_ratio)
        uniform_count = sample_count - importance_count

        # Importance-based sampling
        importance_flat = importance_map.flatten()
        probs = importance_flat / importance_flat.sum()
        importance_indices = np.random.choice(
            H * W,
            size=importance_count,
            replace=False,
            p=probs
        )

        # Uniform sampling
        uniform_indices = np.random.choice(
            H * W,
            size=uniform_count,
            replace=False
        )

        # Combine and convert to UV
        all_indices = np.concatenate([importance_indices, uniform_indices])
        v_coords = all_indices // W
        u_coords = all_indices % W

        coords = np.stack([
            u_coords / W,
            v_coords / H
        ], axis=1)

        return coords
```

**Mask Update Scheduler:**

```python
# sgaps/core/mask_updater.py (TO BE IMPLEMENTED)

class MaskUpdateScheduler:
    """
    Determines when to update sampling mask based on reconstruction quality.

    Strategies:
    - Fixed: Never update (current implementation)
    - Interval: Update every N frames
    - Quality-based: Update when quality drops below threshold
    - Adaptive: Combine interval and quality
    """

    def __init__(self, config):
        self.strategy = config.mask_update.strategy
        self.update_interval = config.mask_update.get('update_interval', 0)
        self.quality_threshold = config.mask_update.get('quality_threshold', 0.95)

    def should_update(self, frame_count, quality_metrics):
        """
        Args:
            frame_count: Current frame number in session
            quality_metrics: dict with 'ssim', 'psnr', etc.

        Returns:
            should_update: bool
        """
        if self.strategy == 'fixed':
            return False
        elif self.strategy == 'interval':
            return frame_count % self.update_interval == 0
        elif self.strategy == 'quality':
            return quality_metrics['ssim'] < self.quality_threshold
        elif self.strategy == 'adaptive':
            interval_trigger = frame_count % self.update_interval == 0
            quality_trigger = quality_metrics['ssim'] < self.quality_threshold
            return interval_trigger or quality_trigger
        else:
            return False
```

---

## ‚ùå PHASE 3: NOT YET IMPLEMENTED

### 3.1 Training Pipeline

#### Missing Files

-   ‚ùå `sgaps/training/` directory (doesn't exist)
-   ‚ùå `sgaps/training/__init__.py`
-   ‚ùå `sgaps/training/trainer.py` - Training loop
-   ‚ùå `sgaps/training/evaluator.py` - Validation and testing
-   ‚ùå `scripts/train.py` - Training CLI
-   ‚ùå `scripts/evaluate.py` - Evaluation CLI

#### Required Components

**Trainer:**

```python
# sgaps/training/trainer.py (TO BE IMPLEMENTED)

class SGAPSTrainer:
    """
    Training pipeline for Sparse Pixel Transformer.

    Features:
    - Curriculum learning: Gradually increase task difficulty
    - Multi-loss optimization: L_sampled + L_perceptual + L_structural + L_temporal
    - WandB integration for experiment tracking
    - Checkpoint management
    """

    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config

        # Losses
        self.loss_sampled = SampledPixelL2Loss()
        # Phase 2+: Add perceptual, structural, temporal losses

        # Optimizer
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.training.learning_rate,
            weight_decay=config.training.weight_decay
        )

        # Scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.training.max_epochs
        )

    def train_epoch(self, epoch):
        """Single training epoch with curriculum learning."""
        pass  # TO BE IMPLEMENTED

    def validate(self):
        """Validation loop with metric computation."""
        pass  # TO BE IMPLEMENTED

    def save_checkpoint(self, epoch, metrics):
        """Save model checkpoint."""
        pass  # TO BE IMPLEMENTED
```

**Evaluator:**

```python
# sgaps/training/evaluator.py (TO BE IMPLEMENTED)

class SGAPSEvaluator:
    """
    Evaluation pipeline for trained models.

    Computes:
    - PSNR, SSIM on validation set
    - Importance map quality
    - Sampling efficiency metrics
    - Visualization of reconstructions
    """

    def __init__(self, model, test_loader, config):
        self.model = model
        self.test_loader = test_loader
        self.config = config

    def evaluate(self):
        """Run full evaluation."""
        pass  # TO BE IMPLEMENTED

    def compute_metrics(self, pred, gt):
        """Compute all metrics for a batch."""
        pass  # TO BE IMPLEMENTED
```

**Training Script:**

```python
# scripts/train.py (TO BE IMPLEMENTED)

import hydra
from omegaconf import DictConfig

@hydra.main(version_base=None, config_path="../conf", config_name="config")
def main(cfg: DictConfig):
    """
    Training entry point.

    Usage:
        python scripts/train.py
        python scripts/train.py training.learning_rate=1e-4
        python scripts/train.py model.embed_dim=512
    """
    # Initialize model
    model = SparsePixelTransformer(cfg)

    # Load dataset
    train_dataset = SGAPSDataset(cfg.data.train_path, cfg)
    val_dataset = SGAPSDataset(cfg.data.val_path, cfg)

    train_loader = DataLoader(train_dataset, ...)
    val_loader = DataLoader(val_dataset, ...)

    # Initialize trainer
    trainer = SGAPSTrainer(model, train_loader, val_loader, cfg)

    # Train
    for epoch in range(cfg.training.max_epochs):
        train_metrics = trainer.train_epoch(epoch)
        val_metrics = trainer.validate()

        # Log to WandB
        # Save checkpoint

if __name__ == "__main__":
    main()
```

### 3.2 Data Augmentation

#### Missing Files

-   ‚ùå `sgaps/data/transforms.py`

#### Required Components

```python
# sgaps/data/transforms.py (TO BE IMPLEMENTED)

class AugmentationPipeline:
    """
    Data augmentation for training robustness.

    Augmentations:
    - Resolution variation (simulating different screen sizes)
    - Noise injection (sensor noise, compression artifacts)
    - Sampling count variation (train with 200-1000 pixels)
    - State vector corruption (missing/noisy game state)
    """

    def __init__(self, config):
        self.config = config

    def __call__(self, frame, pixels, state_vector):
        """Apply random augmentations."""
        pass  # TO BE IMPLEMENTED
```

---

## ‚ùå PHASE 4: ADVANCED FEATURES (Future)

### 4.1 Optical Flow & Motion Prediction

**Not implemented:**

-   Motion vector estimation
-   Future frame prediction
-   Latency compensation

### 4.2 Multi-Checkpoint Management

**Partially implemented:**

-   Checkpoint key system exists
-   `load_checkpoint()` stub in place
-   **Missing:** Actual checkpoint loading, switching, management

### 4.3 Reconstruction Quality Feedback Loop

**Not implemented:**

-   Quality analyzer
-   Uncertainty estimation (explicitly avoiding Monte Carlo Dropout)
-   Adaptive coordinate generation based on quality

### 4.4 Visualization Tools

> **Note:** ÏïÑÎûòÏùò ÏãúÍ∞ÅÌôî Í∏∞Îä•Îì§ÏùÄ **Weights & Biases (WandB)** ÎåÄÏãúÎ≥¥ÎìúÎ•º ÌÜµÌï¥ Íµ¨ÌòÑÎê† ÏòàÏ†ïÏûÖÎãàÎã§.

#### Missing Files

-   ‚ùå `sgaps/utils/visualization.py`

#### Required Features

-   Attention map visualization
-   Reconstruction quality comparison
-   Importance map heatmaps
-   Training progress plots

### 4.5 Unity Debug UI

**Not implemented:**

-   Runtime debug panel
-   Real-time metric display
-   Reconstructed frame visualization
-   Network statistics

---

## üìã IMMEDIATE NEXT STEPS

### Priority 1: Core Model Implementation (Week 5)

1. **Create model directory structure:**

    ```bash
    mkdir sgaps-server/sgaps/models
    touch sgaps-server/sgaps/models/__init__.py
    touch sgaps-server/sgaps/models/spt.py
    touch sgaps-server/sgaps/models/positional_encoding.py
    touch sgaps-server/sgaps/models/losses.py
    ```

2. **Implement SparsePixelTransformer:**

    - Pixel embedding with positional encoding
    - Self-attention encoder
    - Cross-attention decoder
    - State vector integration
    - CNN refinement head

3. **Implement loss functions:**
    - Start with SampledPixelL2Loss (MVP)
    - Test on dummy data

### Priority 2: Training Pipeline (Week 5-6)

4. **Create training directory:**

    ```bash
    mkdir sgaps-server/sgaps/training
    touch sgaps-server/sgaps/training/__init__.py
    touch sgaps-server/sgaps/training/trainer.py
    ```

5. **Implement basic trainer:**

    - Simple training loop
    - Checkpoint saving/loading
    - Validation metrics

6. **Create training script:**
    ```bash
    mkdir sgaps-server/scripts
    touch sgaps-server/scripts/train.py
    ```

### Priority 3: Adaptive Sampling (Week 6)

7. **Implement importance calculation:**

    - Extract attention weights from model
    - Calculate entropy-based importance
    - Generate importance maps

8. **Extend sampler:**

    - Importance-based weighted sampling
    - Hybrid sampling (60% importance + 40% uniform)

9. **Implement mask updater:**
    - Quality-based update strategy
    - Interval-based strategy

---

## üîß CONFIGURATION REQUIREMENTS

### Add to `conf/config.yaml`:

```yaml
# Model configuration
model:
    embed_dim: 256
    num_heads: 8
    num_encoder_layers: 6
    num_decoder_layers: 4
    feedforward_dim: 1024
    dropout: 0.1
    color_channels: 1 # Grayscale for Phase 1

# Training configuration
training:
    learning_rate: 1e-4
    weight_decay: 1e-5
    max_epochs: 100
    batch_size: 16
    accumulation_steps: 1
    warmup_epochs: 10

    # Curriculum learning
    curriculum:
        phase1_epochs: 30 # Easy: 1000 samples
        phase2_epochs: 40 # Medium: 500 samples
        phase3_epochs: 30 # Hard: 250 samples

# Loss weights
loss:
    sampled_weight: 1.0
    perceptual_weight: 0.0 # Phase 2+
    structural_weight: 0.0 # Phase 2+
    temporal_weight: 0.0 # Phase 2+

# Importance calculation
importance:
    method: attention_entropy
    epsilon: 1e-9

# Data paths
data:
    train_path: ./data/train
    val_path: ./data/val
    test_path: ./data/test
```

---

## üìä ESTIMATED COMPLETION TIMELINE

| Task                      | Estimated Time | Dependencies                  |
| ------------------------- | -------------- | ----------------------------- |
| **SPT Model**             | 3-5 days       | None                          |
| **Loss Functions**        | 1 day          | SPT Model                     |
| **Training Pipeline**     | 2-3 days       | SPT Model, Loss Functions     |
| **Importance Calculator** | 1-2 days       | SPT Model (attention weights) |
| **Adaptive Sampler**      | 1 day          | Importance Calculator         |
| **Mask Updater**          | 1 day          | None                          |
| **Training Scripts**      | 1 day          | Training Pipeline             |
| **Data Augmentation**     | 1-2 days       | None                          |
| **Visualization Tools**   | 2-3 days       | Optional                      |

**Total Estimated Time: 2-3 weeks for Phase 2-3 completion**

---

## üìà SUCCESS CRITERIA

### Phase 2 Completion

-   ‚úÖ SPT model implemented and trainable
-   ‚úÖ Basic training pipeline working
-   ‚úÖ Can train on collected data
-   ‚úÖ Attention-based importance maps generated
-   ‚úÖ Adaptive sampling working

### Phase 3 Completion

-   ‚úÖ Model achieves >35dB PSNR on validation set
-   ‚úÖ Importance-based sampling outperforms random sampling
-   ‚úÖ Can load trained checkpoints in server
-   ‚úÖ Real-time inference (<10ms per frame on GPU)
-   ‚úÖ Full documentation updated

---

## üêõ KNOWN ISSUES

### Documentation vs Implementation Mismatch

1. **README.md claims complete system:**

    - Shows detailed architecture diagrams for unimplemented features
    - Performance benchmarks are aspirational, not measured
    - **Action:** Add "Status: Planned" markers

2. **SERVER_IMPLEMENTATION.md shows full code:**

    - Includes example code for `SparsePixelTransformer` that doesn't exist
    - Shows training pipeline that isn't implemented
    - **Action:** Mark sections as "TO BE IMPLEMENTED"

3. **API spec assumes model exists:**
    - `session_start_ack` includes `model_version` and `checkpoint_loaded`
    - Currently always sends `model_version: "v1.0.0"` and `checkpoint_loaded: false`
    - **Action:** Document as placeholder

### Technical Debt

1. **SSIM implementation is simplified:**

    - Should use `scikit-image.metrics.structural_similarity`
    - Current version in `sgaps/utils/metrics.py` is placeholder

2. **Checkpoint loading is stub:**

    - `reconstructor.load_checkpoint()` always returns False
    - Need actual model checkpoint management

3. **No error recovery in client:**
    - If server crashes during session, client doesn't reconnect
    - Need automatic reconnection logic

---

## üìù DOCUMENTATION UPDATES NEEDED

1. **README.md:**

    - Add "Implementation Status" section
    - Mark aspirational features clearly
    - Update performance benchmarks to "Projected"

2. **SERVER_IMPLEMENTATION.md:**

    - Add implementation status to each section
    - Mark code examples as "Planned Architecture"
    - Add current stub implementations

3. **CLIENT_IMPLEMENTATION.md:**

    - Mark debug UI as "Not Implemented"
    - Document actual frame capture method (ScreenCapture, not Camera)

4. **DEVELOPMENT_ROADMAP.md:**
    - Update with actual completion status
    - Add estimated timeline for remaining phases

---

## üéØ CONCLUSION

**Current State: Infrastructure Complete, ML Core Pending**

The SGAPS-MAE project has a **rock-solid foundation** with all Phase 1 goals achieved:

-   ‚úÖ Server-client communication working flawlessly
-   ‚úÖ Data collection pipeline operational
-   ‚úÖ HDF5 storage organized and efficient
-   ‚úÖ Configuration system flexible and complete
-   ‚úÖ Unity integration clean and non-intrusive

**However, the core innovation (30% ‚Üí 70% of project) is not yet implemented:**

-   ‚ùå Sparse Pixel Transformer model
-   ‚ùå Adaptive sampling system
-   ‚ùå Training pipeline
-   ‚ùå Attention-based importance

**The project is ready to move to Phase 2/3 immediately.** All infrastructure is in place, and implementing the ML components is now unblocked.

**Recommendation:** Focus on SPT model implementation first, as it's the critical path for all other features (importance calculation, adaptive sampling require the model).

---

**Status Legend:**

-   ‚úÖ Fully Implemented
-   ‚ö†Ô∏è Partially Implemented (Stub/Placeholder)
-   ‚ùå Not Implemented
-   üî¥ Not Started
-   üü° In Progress
-   üü¢ Complete

---

_This document will be updated as implementation progresses._
